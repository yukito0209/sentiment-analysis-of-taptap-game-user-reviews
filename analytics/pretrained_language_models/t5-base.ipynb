{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d7d406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5TokenizerFast,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import evaluate # Hugging Face's evaluate library\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d2cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 配置 ---\n",
    "data_path = r\"D:\\GitHubRepos\\is6941-ml-social-media\\taptap\\data\\integrated\\lm_cleaned_taptap_reviews.csv\"\n",
    "model_name = \"google-t5/t5-base\"\n",
    "text_column = \"review_content\"\n",
    "label_column = \"sentiment\"\n",
    "test_size = 0.2  # 20% 的数据作为测试集\n",
    "random_state = 42 # 为了结果可复现\n",
    "max_input_length = 256 # T5 输入序列最大长度\n",
    "max_target_length = 8   # 目标序列最大长度 (\"positive\", \"negative\")\n",
    "train_batch_size = 8    # 根据你的 GPU 显存调整\n",
    "eval_batch_size = 16   # 根据你的 GPU 显存调整\n",
    "num_train_epochs = 3    # 训练轮数\n",
    "learning_rate = 5e-5\n",
    "output_dir = \"./t5_sentiment_results\" # 训练结果输出目录\n",
    "logging_dir = \"./t5_sentiment_logs\"   # 日志目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8d62157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully. Shape: (39985, 2)\n",
      "Sample data:\n",
      "                   review_content  sentiment\n",
      "0            可以体验一下，剧情不错，但可能会有点迷          1\n",
      "1                           剧情很好          1\n",
      "2  刺激，感受到了友情，亲情，自我，爱慕，传承，等待 ，与纯真          1\n",
      "3            毋庸置疑的神作 第一次回溯时间的是葵啊          1\n",
      "4                   没有别的可以说，剧情神作          1\n",
      "\n",
      "Label distribution:\n",
      " sentiment\n",
      "1    26353\n",
      "0    13632\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 加载数据 ---\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    df = pd.read_csv(data_path, usecols=[text_column, label_column])\n",
    "    # 处理可能的缺失值\n",
    "    df = df.dropna(subset=[text_column, label_column])\n",
    "    # 确保标签是整数类型\n",
    "    df[label_column] = df[label_column].astype(int)\n",
    "    print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "    print(\"Sample data:\\n\", df.head())\n",
    "    print(\"\\nLabel distribution:\\n\", df[label_column].value_counts())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Data file not found at {data_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b028de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n",
      "Preprocessing complete. Sample processed data:\n",
      "                                           input_text target_text\n",
      "0            classify sentiment: 可以体验一下，剧情不错，但可能会有点迷    positive\n",
      "1                           classify sentiment: 剧情很好    positive\n",
      "2  classify sentiment: 刺激，感受到了友情，亲情，自我，爱慕，传承，等待 ，与纯真    positive\n",
      "3            classify sentiment: 毋庸置疑的神作 第一次回溯时间的是葵啊    positive\n",
      "4                   classify sentiment: 没有别的可以说，剧情神作    positive\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 预处理数据 ---\n",
    "print(\"Preprocessing data...\")\n",
    "# T5 需要文本标签\n",
    "label_map = {0: \"negative\", 1: \"positive\"}\n",
    "# 反向映射，用于评估时解码\n",
    "id_to_label = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# 添加任务前缀并转换标签\n",
    "df['input_text'] = \"classify sentiment: \" + df[text_column]\n",
    "df['target_text'] = df[label_column].map(label_map)\n",
    "\n",
    "# 检查是否有标签未能成功映射\n",
    "if df['target_text'].isnull().any():\n",
    "    print(\"Warning: Some labels could not be mapped. Check your label_column values and label_map.\")\n",
    "    print(\"Rows with null target_text:\", df[df['target_text'].isnull()])\n",
    "    df = df.dropna(subset=['target_text']) # 移除无法映射的行\n",
    "\n",
    "print(\"Preprocessing complete. Sample processed data:\\n\", df[['input_text', 'target_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc75438e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data...\n",
      "Train set size: 31988\n",
      "Test set size: 7997\n"
     ]
    }
   ],
   "source": [
    "# --- 3. 划分数据集 ---\n",
    "print(\"Splitting data...\")\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=test_size,\n",
    "    random_state=random_state,\n",
    "    stratify=df[label_column] # 保持训练集和测试集标签分布一致\n",
    ")\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a4d511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to Hugging Face Datasets...\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_text', 'target_text', '__index_level_0__'],\n",
      "        num_rows: 31988\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_text', 'target_text', '__index_level_0__'],\n",
      "        num_rows: 7997\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# --- 4. 转换为 Hugging Face Datasets ---\n",
    "print(\"Converting to Hugging Face Datasets...\")\n",
    "train_dataset = Dataset.from_pandas(train_df[['input_text', 'target_text']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['input_text', 'target_text']])\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b473800d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model: google-t5/t5-base...\n",
      "GPU is available. Using CUDA.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. 加载 Tokenizer 和模型 ---\n",
    "print(f\"Loading tokenizer and model: {model_name}...\")\n",
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 检查 GPU 是否可用\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available. Using CUDA.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# 将模型移到正确的设备 (Trainer 会自动处理，但显式检查有益)\n",
    "# model.to(device) # Trainer 会自动处理设备放置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16d8bed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7898b54096be4cedaa21f24a43b57a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/31988 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wjw20\\miniconda3\\envs\\IS6941\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57303f53e606450f867ffb3691f22487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7997 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete.\n",
      "Sample tokenized input: [853, 4921, 6493, 10, 3, 2, 6, 2, 6, 2, 6, 2, 1808, 2, 6, 2, 6, 2, 6, 2, 3, 14817, 14817, 18, 14817, 14817, 3, 2, 599, 2, 2773, 2, 61, 3, 2, 6, 2, 3, 2, 6, 2, 6, 2, 6, 52, 157, 7, 2, 6, 2, 3, 2, 52, 157, 7, 2, 6, 2, 536, 15938, 2, 6, 2, 1808, 5, 927, 2, 6, 2, 6, 2, 52, 157, 7, 2, 3, 2, 6, 2, 6, 2, 6, 2, 2517, 5, 927, 2, 6, 2, 2606, 2, 3, 2, 6, 2, 6, 2, 6, 2, 6, 2, 3, 2, 6, 2, 6, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sample tokenized label: [1465, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Tokenize 数据 ---\n",
    "print(\"Tokenizing data...\")\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['input_text']\n",
    "    targets = examples['target_text']\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset_dict.map(preprocess_function, batched=True)\n",
    "print(\"Tokenization complete.\")\n",
    "print(\"Sample tokenized input:\", tokenized_datasets['train'][0]['input_ids'])\n",
    "print(\"Sample tokenized label:\", tokenized_datasets['train'][0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44ddcedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training arguments...\n"
     ]
    }
   ],
   "source": [
    "# --- 7. 设置训练参数 ---\n",
    "print(\"Setting up training arguments...\")\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    eval_strategy=\"epoch\",      # 每个 epoch 结束后进行评估\n",
    "    save_strategy=\"epoch\",            # 每个 epoch 结束后保存模型\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,      # 训练结束后加载最佳模型\n",
    "    metric_for_best_model=\"eval_loss\",# 使用评估损失来判断最佳模型\n",
    "    predict_with_generate=True,       # 在评估时使用 generate 方法生成文本\n",
    "    logging_dir=logging_dir,\n",
    "    logging_steps=100,                 # 每 100 步记录一次日志\n",
    "    fp16=torch.cuda.is_available(),   # 如果有 GPU，使用混合精度训练加速\n",
    "    # report_to=\"tensorboard\" # 可以取消注释以使用 TensorBoard\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2777caed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining compute metrics function...\n"
     ]
    }
   ],
   "source": [
    "# --- 8. 定义评估指标 ---\n",
    "print(\"Defining compute metrics function...\")\n",
    "# 获取真实标签的数值形式，用于 sklearn metrics\n",
    "# 注意：确保 test_df 在这个作用域内是可访问的，或者在调用时传递它\n",
    "# 或者，更好的方法是从原始的 test_dataset 中提取标签\n",
    "# 为了简单起见，我们假设 test_df 在这里仍然可用\n",
    "true_labels_numeric = test_df[label_column].tolist()\n",
    "# 获取文本标签列表，用于 classification_report\n",
    "text_label_list = list(label_map.values()) # [\"negative\", \"positive\"]\n",
    "\n",
    "# 创建反向映射\n",
    "text_to_id_map = {v: k for k, v in label_map.items()} # {\"negative\": 0, \"positive\": 1}\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred # labels 在这里是 token IDs，我们不需要它们，因为我们有 true_labels_numeric\n",
    "\n",
    "    # predictions 是 token IDs, 需要解码\n",
    "    # T5 的 generate 输出通常包含 pad token ID，即使设置了 skip_special_tokens\n",
    "    # 替换 pad token ID 为 -100 以便解码器忽略它们 (虽然 skip_special_tokens 应该处理)\n",
    "    # predictions[predictions == tokenizer.pad_token_id] = -100 # 通常不需要，skip_special_tokens=True 足够\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # *** 这是修改的关键部分 ***\n",
    "    # 将解码后的文本 (\"negative\", \"positive\") 转换回 0, 1\n",
    "    # 使用 .strip() 处理可能的首尾空格\n",
    "    pred_labels_numeric = [text_to_id_map.get(pred.strip(), -1) for pred in decoded_preds]\n",
    "    # 如果解码结果不在 text_to_id_map 中 (例如模型生成了奇怪的东西)，则映射为 -1\n",
    "\n",
    "    # 过滤掉无法识别的预测标签 (-1)\n",
    "    # 同时确保我们只比较那些有有效预测的真实标签\n",
    "    valid_indices = [i for i, label in enumerate(pred_labels_numeric) if label != -1]\n",
    "\n",
    "    # 检查过滤后是否还有样本\n",
    "    if not valid_indices:\n",
    "        print(\"Warning: No valid predictions found after decoding/filtering. Check model outputs.\")\n",
    "        print(\"Sample Decoded Predictions (first 10):\", decoded_preds[:10]) # 打印解码输出来调试\n",
    "        return {\"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "\n",
    "    filtered_preds = [pred_labels_numeric[i] for i in valid_indices]\n",
    "    # 确保使用相同索引过滤真实标签\n",
    "    filtered_true = [true_labels_numeric[i] for i in valid_indices]\n",
    "\n",
    "    print(\"\\n--- Evaluation ---\")\n",
    "    print(\"Sample Decoded Predictions:\", decoded_preds[:5])\n",
    "    print(\"Sample Predicted Numeric Labels (filtered):\", filtered_preds[:5])\n",
    "    print(\"Sample True Numeric Labels (filtered):\", filtered_true[:5])\n",
    "\n",
    "    # 计算 classification report\n",
    "    # 确保 target_names 的顺序与标签 0, 1 对应\n",
    "    target_names_for_report = [label_map[0], label_map[1]] # [\"negative\", \"positive\"]\n",
    "    try:\n",
    "        report = classification_report(filtered_true, filtered_preds, target_names=target_names_for_report, output_dict=True, zero_division=0)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(filtered_true, filtered_preds, target_names=target_names_for_report, zero_division=0))\n",
    "\n",
    "        # 计算 confusion matrix\n",
    "        # labels 参数指定了矩阵的行/列顺序，确保与 target_names 一致\n",
    "        cm = confusion_matrix(filtered_true, filtered_preds, labels=[0, 1])\n",
    "        print(\"\\nConfusion Matrix (Rows: True, Cols: Pred):\")\n",
    "        print(f\"       {label_map[0]}  {label_map[1]}\")\n",
    "        print(f\"{label_map[0]}: {cm[0]}\")\n",
    "        print(f\"{label_map[1]}: {cm[1]}\")\n",
    "        print(\"------------------\")\n",
    "\n",
    "        # 返回主要的指标给 Trainer\n",
    "        return {\n",
    "            \"accuracy\": report[\"accuracy\"],\n",
    "            \"precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"recall\": report[\"weighted avg\"][\"recall\"],\n",
    "            \"f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics: {e}\")\n",
    "        print(\"Filtered True Labels:\", filtered_true)\n",
    "        print(\"Filtered Pred Labels:\", filtered_preds)\n",
    "        return {\"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34c6f986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wjw20\\AppData\\Local\\Temp\\ipykernel_27968\\1316736279.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# --- 9. 初始化 Trainer ---\n",
    "print(\"Initializing Trainer...\")\n",
    "# 数据整理器，负责动态填充批次中的序列\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    padding=\"max_length\", # 确保标签也被填充\n",
    "    max_length=max_input_length,\n",
    "    label_pad_token_id=tokenizer.pad_token_id # 明确指定标签填充 ID\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee17a65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11997' max='11997' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11997/11997 44:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>0.483306</td>\n",
       "      <td>0.634048</td>\n",
       "      <td>0.483306</td>\n",
       "      <td>0.468555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.670126</td>\n",
       "      <td>0.643272</td>\n",
       "      <td>0.670126</td>\n",
       "      <td>0.589953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.670001</td>\n",
       "      <td>0.643019</td>\n",
       "      <td>0.670001</td>\n",
       "      <td>0.589581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation ---\n",
      "Sample Decoded Predictions: ['negative', 'positive', 'negative', 'negative', 'negative']\n",
      "Sample Predicted Numeric Labels (filtered): [0, 1, 0, 0, 0]\n",
      "Sample True Numeric Labels (filtered): [1, 1, 0, 1, 1]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.38      0.82      0.52      2726\n",
      "    positive       0.77      0.31      0.44      5271\n",
      "\n",
      "    accuracy                           0.48      7997\n",
      "   macro avg       0.57      0.56      0.48      7997\n",
      "weighted avg       0.63      0.48      0.47      7997\n",
      "\n",
      "\n",
      "Confusion Matrix (Rows: True, Cols: Pred):\n",
      "       negative  positive\n",
      "negative: [2223  503]\n",
      "positive: [3629 1642]\n",
      "------------------\n",
      "\n",
      "--- Evaluation ---\n",
      "Sample Decoded Predictions: ['positive', 'positive', 'positive', 'positive', 'positive']\n",
      "Sample Predicted Numeric Labels (filtered): [1, 1, 1, 1, 1]\n",
      "Sample True Numeric Labels (filtered): [1, 1, 0, 1, 1]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.12      0.20      2726\n",
      "    positive       0.68      0.95      0.79      5271\n",
      "\n",
      "    accuracy                           0.67      7997\n",
      "   macro avg       0.63      0.54      0.50      7997\n",
      "weighted avg       0.64      0.67      0.59      7997\n",
      "\n",
      "\n",
      "Confusion Matrix (Rows: True, Cols: Pred):\n",
      "       negative  positive\n",
      "negative: [ 327 2399]\n",
      "positive: [ 239 5032]\n",
      "------------------\n",
      "\n",
      "--- Evaluation ---\n",
      "Sample Decoded Predictions: ['positive', 'positive', 'positive', 'positive', 'positive']\n",
      "Sample Predicted Numeric Labels (filtered): [1, 1, 1, 1, 1]\n",
      "Sample True Numeric Labels (filtered): [1, 1, 0, 1, 1]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.12      0.20      2726\n",
      "    positive       0.68      0.95      0.79      5271\n",
      "\n",
      "    accuracy                           0.67      7997\n",
      "   macro avg       0.63      0.54      0.49      7997\n",
      "weighted avg       0.64      0.67      0.59      7997\n",
      "\n",
      "\n",
      "Confusion Matrix (Rows: True, Cols: Pred):\n",
      "       negative  positive\n",
      "negative: [ 325 2401]\n",
      "positive: [ 238 5033]\n",
      "------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  total_flos               = 27212330GF\n",
      "  train_loss               =     0.0195\n",
      "  train_runtime            = 0:44:16.21\n",
      "  train_samples_per_second =     36.128\n",
      "  train_steps_per_second   =      4.517\n"
     ]
    }
   ],
   "source": [
    "# --- 10. 训练 ---\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# 保存最终模型和 tokenizer\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# 记录训练指标\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ff3c848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on the test set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 01:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluation ---\n",
      "Sample Decoded Predictions: ['positive', 'positive', 'positive', 'positive', 'positive']\n",
      "Sample Predicted Numeric Labels (filtered): [1, 1, 1, 1, 1]\n",
      "Sample True Numeric Labels (filtered): [1, 1, 0, 1, 1]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.58      0.12      0.20      2726\n",
      "    positive       0.68      0.95      0.79      5271\n",
      "\n",
      "    accuracy                           0.67      7997\n",
      "   macro avg       0.63      0.54      0.50      7997\n",
      "weighted avg       0.64      0.67      0.59      7997\n",
      "\n",
      "\n",
      "Confusion Matrix (Rows: True, Cols: Pred):\n",
      "       negative  positive\n",
      "negative: [ 327 2399]\n",
      "positive: [ 239 5032]\n",
      "------------------\n",
      "\n",
      "--- Final Test Set Evaluation Results ---\n",
      "Evaluation Loss: 0.002551098819822073\n",
      "Accuracy: 0.6701262973615105\n",
      "Precision: 0.6432715526495508\n",
      "Recall: 0.6701262973615105\n",
      "F1-Score: 0.5899531119508825\n"
     ]
    }
   ],
   "source": [
    "# --- 11. 评估 ---\n",
    "print(\"Starting evaluation on the test set...\")\n",
    "eval_metrics = trainer.evaluate()\n",
    "\n",
    "print(\"\\n--- Final Test Set Evaluation Results ---\")\n",
    "print(f\"Evaluation Loss: {eval_metrics.get('eval_loss', 'N/A')}\")\n",
    "print(f\"Accuracy: {eval_metrics.get('eval_accuracy', 'N/A')}\")\n",
    "print(f\"Precision: {eval_metrics.get('eval_precision', 'N/A')}\")\n",
    "print(f\"Recall: {eval_metrics.get('eval_recall', 'N/A')}\")\n",
    "print(f\"F1-Score: {eval_metrics.get('eval_f1', 'N/A')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IS6941",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
